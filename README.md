# awesome-3dgs-avatar
**Awesome 3D Gaussian Splatting ‚Üí Avatars & Dynamic Humans**

A curated list of papers, implementations, datasets, demos, and resources focusing on **3D Gaussian Splatting (3DGS)** methods applied to **avatars / dynamic human modeling**: head avatars, full-body clothed avatars, expression & pose control, single-image / video / multi-view inputs, and real-time rendering.

‚≠ê If you like this list, please give it a star! üòÑ


[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](#license)  
[![Last Updated](https://img.shields.io/badge/last%20updated-2025--11--02-blue.svg)](#)  
[![Contributions Welcome](https://img.shields.io/badge/Contributions-Welcome-green.svg)](#contributing)  

 
---

## üöÄ Table of Contents

- [Papers](#papers)
  - [Head and Face Avatars](#head--face-avatars)
  - [Full-Body and Clothed Avatars](#full--upper-clothed-avatars)
  - [Pose and Expression Control](#pose--expression-control)
  - [3D/4D Generation with Gaussian Splatting](#3d4d-generation-with-gaussian-splatting)
- [Implementations and Code](#implementations--code)
- [Datasets](#datasets)
- [Demos and Videos](#demos--videos)
- [Surveys and Related Resources](#surveys--related-resources)
- [Scope and What's Included](#scope--whats-included)
- [Contributing](#contributing)
- [License](#license)

---

## üìöPapers

### Head & Face Avatars

| Title | Model Input | What is Controllable | Speed | Papers & Codes | Contribution | Views Type | specified problem | Training & Inference Device | Limitations |
|---|---|---|---|---|---|---|---|---|---|
| **(NeurIPS24)Generalizable and Animatable Gaussian Head Avatar** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2410.07971)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/xg-chu/GAGAvatar)  | | |
| **(CVPR24)Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2312.03029)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/YuelangX/Gaussian-Head-Avatar)  | | |
| **(25)SEGA: Drivable 3D Gaussian Head Avatar from a Single Image** | Single image; using UV-space Gaussian framework + FLAME prior | Expression + view + identity; generalizes to unseen identities | Real-time performance claimed | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/abs/2504.14373) Not release code|
| **(CVPR24 Highlight)GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians** |  | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2312.02069) [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/ShenhanQian/GaussianAvatars)| |
| **(CVPR25)3D Gaussian Head Avatars with Expressive Dynamic Appearances by Compact Tensorial Representations** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/abs/2504.14967)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/ant-research/TensorialGaussianAvatar)  | | |
| **(CVPR25)Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID Guidance** |A single facial image. (It can also compute an average identity embedding from multiple images )|Facial expressions, which are driven by 3DMM (FLAME) blendshapes.|Approximately 80 minutes on one GPU. Rendering: Real-time| [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/abs/2501.05379)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/dimgerogiannis/Arc2Avatar)  |1. The first SDS-based method to generate realistic 3D avatars from a single image using a face foundation model (Arc2Face) for guidance. 2. A 3DGS avatar representation that is controllable via blendshapes by maintaining dense correspondence with a 3DMM (FLAME) template. 3. An optional SDS-based refinement step to correct and enhance expressions.|Generates full 3D 360¬∞ head avatars, including frontal, side, and back views|
| **(CVPR25)AvatarArtist: Open-Domain 4D Avatarization** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2503.19906)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/ant-research/AvatarArtist)  | | |
| **(CVPR25 Oral)CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models** |Any number of reference images (e.g., 1 to 100+). It can also accept images generated from text-to-image models or 2D-edited images (e.g., virtual makeup).|The generated 4D avatar's pose and expression|Rendering: The final 4D avatar supports real-time animation and rendering . Generation/Training: Stage 1 (MMDM image generation) takes ~4 hours (on 4x RTX6000 GPUs). Stage 2 (4D avatar reconstruction/fitting) takes ~4 hours (on 1x RTX6000 GPU)| [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/abs/2412.12093)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/felixtaubner/cap4d/)  |1. A Morphable Multi-View Diffusion Model (MMDM) for portrait generation with pose and expression control. 2. A stochastic I/O conditioning procedure that allows the model to use an arbitrary number of reference images (1-100+) and generate hundreds of self-consistent novel views. 3. A technique to distill the generated images into a real-time, animatable 4D avatar based on 3D Gaussian Splatting|The MMDM is trained on both monocular and multi-view videos and generates hundreds of novel viewpoints.|
| **(CVPR25)FATE: Full-head Gaussian Avatar with Textural Editing from Monocular Video** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/abs/2411.15604)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/zjwfufu/FateAvatar)  | | |
| **(CVPR25)GPAvatar: High-fidelity Head Avatars by Learning Efficient Gaussian Projections** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://openaccess.thecvf.com/content/CVPR2025/papers/Feng_GPAvatar_High-fidelity_Head_Avatars_by_Learning_Efficient_Gaussian_Projections_CVPR_2025_paper.pdf)<br>  Not Release Code  | | |
| **(CVPR25)HERA: Hybrid Explicit Representation for Ultra-Realistic Head Avatars** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2403.11453)<br>  Not Release Code  | | |
| **(CVPR24)FlashAvatar: High-fidelity Head Avatar with Efficient Gaussian Embedding** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2312.02214)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/USTC3DV/FlashAvatar-code)  | | |
| **(CVPR24)ASH: Animatable Gaussian Splats for Efficient and Photoreal Human Rendering** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2312.05941)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/kv2000/ASH)  | | |
| **(CVPR25)HRAvatar: High-Quality and Relightable Gaussian Head Avatar** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2503.08224)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/Pixel-Talk/HRAvatar)  | | |
| **(CVPR25)LUCAS: Layered Universal Codec Avatars** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2502.19739)<br>  Not Release Code  | | |
| **(CVPR25)MeGA: Hybrid Mesh-Gaussian Head Avatar for High-Fidelity Rendering and Head Editing** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2404.19026)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/conallwang/MeGA)  | | |
| **(CVPR25)MobilePortrait: Real-Time One-Shot Neural Head Avatars on Mobile Devices** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2407.05712)<br>  Not Release Code  | | |
| **(CVPR25)PERSE: Personalized 3D Generative Avatars from A Single Portrait** |A single reference portrait image|Animatable head pose and facial expressions|Training: Computationally intensive. Requires ~1.5 days on 8x RTX A6000 GPUs to train the avatar model for each new identity.| [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2412.21206)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/snuvclab/perse?tab=readme-ov-file)  | | |
| **(CVPR25)RGBAvatar: Reduced Gaussian Blendshapes for Online Modeling of Head Avatars** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2503.12886)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/gapszju/RGBAvatar)  | | |
| **(CVPR25)Synthetic Prior for Few-Shot Drivable Head Avatar Inversion** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2501.06903)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/Zielon/synshot) Not Release Code | | |
| **(NeurIPS25)Low-Rank Head Avatar Personalization with Registers** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2506.01935)<br>  Not Release Code | | |
| **(NeurIPS25)BecomingLit: Relightable Gaussian Avatars with Hybrid Neural Shading** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2506.06271)<br>  Not Release Code | | |
| **(ICCV25)FaceCraft4D: Animated 3D Facial Avatar Generation from a Single Image** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2504.15179)<br>  Not Release Code | | |
| **(ICCV25)StrandHead: Text to Hair-Disentangled 3D Head Avatars Using Human-Centric Priors** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://github.com/XiaokunSun/StrandHead)<br>  Not Release Code | | |
| **(ICCV25)TeRA : Rethinking Text-guided Realistic 3D Avatar Generation** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://zjwsite.github.io/files/TeRA.pdf)<br>  Not Release Code | | |
| **(ICCV25)Large Animatable Gaussian Reconstruction Model for High-fidelity 3D Head Avatars** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://tobias-kirschstein.github.io/avat3r/)<br>  Not Release Code | | |
| **(ICCV25)InteractAvatar: Modeling Hand-Face Interaction in Photorealistic Avatars with Deformable Gaussians** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2504.07949)<br>  Not Release Code | | |
| **(ICCV25)GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2507.18155)<br>  Not Release Code | | |
| **(NeurIPS25)CGS-GAN: 3D Consistent Gaussian Splatting GANs for High Resolution Human Head Synthesis** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2505.17590)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/fraunhoferhhi/cgs-gan) | | | ÔΩúTraining: 4x Nvidia H100 GPUs. Inference (Synthesis Speed): 1x Nvidia 4090ÔΩú






---

### Full / Upper Clothed Avatars

| Title | Model Input | What is Controllable | Speed | Papers & Codes | Contribution | Views Type | specified problem | Training & Inference Device | Limitations |
|---|---|---|---|---|---|---|---|---|---|
|**(Arxiv24)CHASE: 3D-Consistent Human Avatars with Sparse Inputs via Gaussian Splatting and Contrastive Learning**| | | |[![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b?style=for-the-badge.svg)](https://arxiv.org/pdf/2408.09663)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2?style=for-the-badge.svg)](https://chaseprojectpage.github.io/)|
| **(MM24)Animatable 3D Gaussian: Fast and High-Quality Reconstruction of Multiple Human Avatars** | video sequence of one or more humans along with their corresponding pose data $(S_t,T_t)$ for each frame. | novel view synthesis and novel pose synthesis | real-time claimed |[![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b?style=for-the-badge.svg)](https://arxiv.org/pdf/2311.16482)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2?style=for-the-badge.svg)](https://github.com/jimmyYliu/Animatable-3d-Gaussian)| 1. Animatable 3D Gaussian: A new representation for dynamic humans that allows for fast, high-quality reconstruction without needing a specific shape prior like SMPL. 2. significantly faster (training in seconds) and uses less memory than previous state-of-the-art methods. 3. Dynamic Shadow Modeling. | monocular (single-view) or sparse multi-view video sequences |
| **(3DV25)D3GA - Drivable 3D Gaussian Avatars** | 3D joint angles and facial keypoints | body, individual garments (like shirts and pants), and the face. | 1024√ó667 resolution<br> it achieves approximately 26 FPS | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2311.08581)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/facebookresearch/D3GA) | 1. A lightweight and composable avatar model using 3D Gaussians that are deformed by tetrahedral cages instead of standard Linear Blend Skinning (LBS). This allows for more natural stretching and re-orientation of the primitives. 2. The ability to use localized conditioning, meaning different parts of the avatar (e.g., face, body) can be driven by different input signals (e.g., keypoints, joint angles). | dense multi-view video captured in a studio setting. 200 cameras as well as a smaller 40-camera setup |
| **(23)Human101: Training 100+FPS Human Gaussians in 100s from 1 View** | A single-view video, pre-calibrated camera parameters, SMPL (human pose and shape) parameters for each frame, and foreground masks. | SMPL pose (Œ∏) and shape (Œ≤)  | Rendering is real-time | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2312.15258.pdf)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/longxiang-ai/Human101) Not release code| 1.  Pioneers the use of 3D Gaussian Splatting (3D GS) for dynamic human reconstruction, leveraging its explicit representation for efficient rendering. 2.  Proposes a Canonical Human Initialization method that creates a high-quality initial model by fusing point clouds, which significantly accelerates convergence. 3.  Introduces a Human-centric Forward Gaussian Animation method that is more efficient than the traditional backward skinning used in NeRF-based approaches, enabling fast pose-driven animation| Monocular |
| **(CVPR24)SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes** | An image sequence from a monocular dynamic video | Scene motion can be edited by manipulating a graph of sparse control points. | high rendering speed | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://yihua7.github.io/SC-GS-web/materials/SC_GS_Arxiv.pdf)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/CVMI-Lab/SC-GS) | 1. Proposes a novel representation that uses sparse control points and an MLP to model the motion of dense 3D Gaussians for dynamic scenes.  2. Introduces an adaptive strategy to adjust the number of control points and an "As-Rigid-As-Possible" (ARAP) loss to ensure plausible and smooth motions.  3. Enables user-controlled motion editing by deforming a control point graph while preserving high-fidelity appearance | Takes monocular video as input to synthesize novel (free-viewpoint) views of the dynamic scene. |
| **(CVPR24)GART: Gaussian Articulated Template Models** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Lei_GART_Gaussian_Articulated_Template_Models_CVPR_2024_paper.pdf)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/JiahuiLei/GART) | | |
| **(CVPR24)HUGS: Human Gaussian Splats** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2311.17910)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/apple/ml-hugs) | | |
| **(CVPR24)HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2402.06149)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/ZhenglinZhou/HeadStudio) | | |
| **(CVPR24)HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian Splatting** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2312.03461)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://nowheretrix.github.io/HiFi4G/) Not Release Code| | |
| **(CVPR24)GoMAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2404.07991)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/wenj/GoMAvatar)| | |
| **(CVPR24)3D Geometry-aware Deformable Gaussian Splatting for Dynamic View Synthesis** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_3D_Geometry-Aware_Deformable_Gaussian_Splatting_for_Dynamic_View_Synthesis_CVPR_2024_paper.pdf#:~:text=Abstract%20In%20this%20paper%2C%20we,representation%20of%20the%203D%20scene)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/zhichengLuxx/GaGS)| | |
| **(SIGGRAPH Asia 24)Robust Dual Gaussian Splatting for Immersive Human-centric Volumetric Videos** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2409.08353)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/HiFi-Human/DualGS)| | |
| **(NeurIPS24)Template-free Articulated Gaussian Splatting for Real-time Reposable Dynamic View Synthesis** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2412.05570)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/dnvtmf/SK_GS)| | |
| **(CVPR24)GauHuman: Articulated Gaussian Splatting from Monocular Human Videos** | Monocular RGB Video (single view video) , along with pre-processed camera parameters, foreground masks, and SMPL parameters. | novel poses for that subject's avatar by providing new SMPL pose parameters. | real-time rendering.<br> Training Speed: around 1-2 minutes on a single GPU. | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2312.02973.pdf)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/skhu101/GauHuman) | 1. Proposes a novel articulated Gaussian Splatting framework for 3D human modeling that achieves both fast training and real-time rendering. 2. Introduces effective pose and LBS (Linear Blend Skinning) refinement modules to capture fine details. 3. Designs a fast optimization strategy using human priors (SMPL) for initialization/pruning and KL-divergence guidance for splitting/cloning Gaussians, plus a new merge operation | Monocular | efficiency bottleneck in creating high-quality 3D human avatars from monocular videos | single NVIDIA RTX 3090 | 1) The current framework is composed of discrete Gaussian spheres, making it difficult to directly extract traditional 3D mesh models. 2) Recovering very fine dynamic details such as clothing wrinkles from monocular videos remains a huge challenge. |
| **(CVPR24)3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting** | A single monocular video, along with a tracked skeleton (fitted SMPL parameters), camera calibration, and foreground masks | pose to create novel animations and the viewpoint for novel view synthesis. | Real-time rendering. Training Time: ~30 mins on a single GPU. | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.09228)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/mikeqzy/3dgs-avatar-release) | 1. The first work to apply 3D Gaussian Splatting for creating animatable human avatars from monocular video.  2. Developed a deformation network with **as-isometric-as-possible regularizations** to handle highly articulated and unseen poses effectively. 3. The first method to simultaneously achieve high-quality rendering, model pose-dependent deformation, fast training (<30 min), and real-time rendering (50+ FPS) in a single framework. | Monocular | NeRFs are too slow for practical applications. | single NVIDIA RTX 3090 GPU | 1.Training Speed:some other grid-based approaches that can train in under 5 minutes. 2.Texture Quality: blurry results on clothing with high-frequency textures or repeating patterns. 3.Geometric Accuracy: does not produce an accurate geometric reconstruction of the avatar's surface. The resulting surface normals can be noisy due to inconsistencies in the depth of the Gaussian splats. |
|**(CVPR24)GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians**| A single monocular RGB video of a person| The body pose of the generated avatar, allowing for realistic animation with out-of-distribution motions | real-time rendering speed of 35 FPS | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.02134)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/aipixel/GaussianAvatar) | 1.  Introduces animatable 3D Gaussians for realistic human avatar modeling from a single video. 2.  Designs a dynamic appearance network with an optimizable feature tensor to capture pose-dependent details like wrinkles. 3.  Proposes a joint optimization of motion and appearance in an end-to-end manner to correct inaccurate motion estimations from monocular videos. | free-viewpoint rendering and novel view synthesis of the animated avatar | information from a single viewpoint is highly limited, and the initial body motion estimations are often inaccurate. | single NVIDIA RTX 3090 GPU | Inaccurate Foreground Segmentation. Difficulty with Loose Clothing | 
| **(CVPR24 Highlight)GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for Real-time Human Novel View Synthesis** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2312.02155)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/aipixel/GPS-Gaussian)| | |
|**(CVPR24)Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human Avatar Modeling**| Multi-view RGB videos of a character and their corresponding SMPL-X pose and shape registrations | pose can be controlled by a driving pose signal | 10 FPS for animation when rendering 1024x1024 images on a single RTX 3090 GPU | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2311.16096v3)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/lizhe00/AnimatableGaussians) | 1.  Animatable Gaussians: A new avatar representation combining 3D Gaussian Splatting with powerful 2D CNNs to create lifelike avatars with high-fidelity, pose-dependent dynamics. 2.  Template-guided Parameterization: A method to learn a character-specific template (even for loose clothing like dresses) and parameterize 3D Gaussians onto 2D maps, making them compatible with 2D networks. 3.  Pose Projection Strategy: A PCA-based technique to project novel driving poses into the distribution of training poses, enabling better generalization and higher-quality synthesis for unseen poses. | Multi-view RGB videos are required for initial training and template creation. Experiments show it can work with sparse-view inputs (e.g.,  3 views) and still achieve high-fidelity results. | creating lifelike, animatable human avatars from RGB videos. | Training: single NVIDIA RTX 4090 GPU. Inference/Evaluation: a single NVIDIA RTX 3090 GPU | Coupled Body and Clothing: The method models the human body and clothes as a single, entangled representation. Requires Multi-View Input: The approach relies on multi-view video input to reconstruct the initial high-quality parametric template. Limited Secondary Motion: The model cannot simulate the physical motion of components that are not directly driven by the body's skeleton and joints. |
|**(NeurIPS24)Expressive Gaussian Human Avatars from Monocular RGB Video**|  | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/abs/2407.03204)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/evahuman/EVA_Official) |
|**(NeurIPS24)Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion Models**| A single RGB image of a person. | Stochastic generation of occluded regions | Inference uses a DDIM scheduler with 50 reverse sampling steps. | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://yuxuan-xue.com/human-3diffusion/paper/human-3diffusion.pdf)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/YuxuanSnow/Human3Diffusion/) | 1. A novel image-conditioned generative 3D Gaussian Splatting (3D-GS) model that leverages strong shape and texture priors from 2D multi-view diffusion models. 2. A sophisticated diffusion sampling process that uses the explicit 3D-GS reconstruction at each step to refine the 2D sampling trajectory, enforcing 3D consistency throughout the generation. | The model generates a full 3D representation, which can be rendered from any arbitrary novel viewpoint. Internally. | Creating realistic, high-fidelity 3D human avatars from a single RGB image | Training: 8 NVIDIA A100 GPUs for approximately 5 days. Inference: Not stated. | 1. Resolution: The model is constrained by the 256√ó256 resolution. 2. Challenging Poses: The model may struggle to reconstruct subjects in extremely challenging or unusual poses accurately. | 
|**(ECCV24)Expressive Whole-Body 3D Gaussian Avatar**| | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/abs/2407.21686)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/mks0601/ExAvatar_RELEASE) |
|**(CVPR24)SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded Gaussian Splatting**| monocular videos, along with corresponding pre-processed data like registered mesh templates (e.g., SMPL-X, FLAME), masks, and camera parameters. | animation and pose are explicitly controlled by the underlying triangle mesh. This makes it compatible with various standard animation techniques, including skeletal animation, blend shapes, and direct mesh editing | 300 FPS on a modern desktop GPU (NVIDIA RTX 3090) and 30 FPS on a mobile device (iPhone 13) | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.05087)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/initialneil/SplattingAvatar) |1.  Introduced a novel hybrid representation for avatars that integrates Gaussian Splatting with meshes, achieving both realism and computational efficiency. 2.  Applied "lifted optimization" to avatar modeling, enabling the joint optimization of Gaussian parameters and their trainable mesh embeddings. 3.  Demonstrated state-of-the-art quality and real-time rendering capabilities, including a practical implementation in the Unity game engine| Monocular | creating photorealistic and animatable human avatars that can be rendered in real-time. | | 1.  The final rendering quality is highly dependent on the motion representation capability of the underlying driving mesh. 2.  The model cannot represent movements of elements not explicitly modeled by the driving mesh, such as the independent motion of clothing or hair. |
|**(ICCV25)GUAVA: Generalizable Upper Body 3D Gaussian Avatar**| A single image of a person. | The avatar can be animated and controlled by pose, facial expression, and hand gesture parameters extracted from a target video or image sequence. | real-time. ‚Ä¢ Reconstruction Time: ~0.1 seconds (~98 ms) from a single tracked image. ‚Ä¢ Animation & Rendering Speed: ~50 FPS (52.21 FPS reported). | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/abs/2505.03351)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/Pixel-Talk/GUAVA) | 1. First Framework: Proposes the first framework for creating a generalizable upper-body 3D Gaussian avatar from a single image.  2. Expressive Human Model (EHM): Introduces the EHM, which combines SMPLX and FLAME, along with an accurate tracking method to provide better priors for reconstruction. 3. Efficient Reconstruction: Leverages inverse texture mapping and projection sampling to enable fast, feed-forward inference for reconstructing detailed "Ubody Gaussians". | single view and supports novel view synthesis, allowing the rendered avatar to be seen from different camera angles. |
|**(NeurIPS24)HumanSplat: Generalizable Single-Image Human Gaussian Splatting with Structure Priors**| A single RGB image of a human | static modeling | Reconstruction: ~9.3 seconds on a single NVIDIA A100 GPU (~9s for the diffusion model, ~0.3s for the reconstruction transformer). Rendering: Exceeds 150 FPS.| [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2406.12459)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/humansplat/humansplat) | 1.  The first generalizable method for high-fidelity human reconstruction from a single image using 3D Gaussian Splatting. 2. An end-to-end framework integrating a 2D diffusion model with a latent reconstruction transformer. 3.  Effectively combines appearance priors (from diffusion) and structural priors (from SMPL) within a single Transformer architecture. 4.  Improves reconstruction fidelity in key areas like the face and hands using a semantics-guided hierarchical loss. | Single-view input. Internally, it synthesizes multi-view latent features to perform the reconstruction | To reconstruct a photorealistic 3D human model from a single image, overcoming the need for multi-view data or time-consuming per-instance optimization, while achieving a state-of-the-art balance between quality and speed. | Training: 8x NVIDIA A100 (40G VRAM) GPUs for approximately 4 days. Inference: A single NVIDIA A100 GPU | 1.  Speed could be further improved for real-time applications on mobile devices. 2.  The output is a static model; making it animatable requires separate post-processing steps.|
|**(CVPR25)AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent Gaussian Reconstruction**| A single human image in an arbitrary pose. | The final 3D avatar can be animated using new human pose conditions (e.g., SMPL-X parameters) during inference. | Avatar Creation: Not real-time. It takes about 5 minutes to generate multi-view images and another 5 minutes for 4DGS optimization, totaling approximately 10 minutes on a single RTX-3090 GPU. Animation/Inference: Yes, real-time. Once the avatar is reconstructed, it can be animated and rendered in real-time. | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2412.02684)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/aigc3d/AniGS) not release code | 1. Proposes a multi-view canonical image generation method using a video model trained on in-the-wild video data, avoiding the need for synthetic 3D rigged datasets. 2. Reformulates the problem of 3D reconstruction from inconsistent images as a 4D reconstruction task, introducing an efficient 4D Gaussian Splatting (4DGS) model to handle view inconsistencies. 3. Achieves high-fidelity, animatable avatar generation from a single image, enabling photorealistic and real-time animation during inference | The model generates multi-view images of the subject in a canonical pose (e.g., T-pose) | Generating a high-fidelity, animatable 3D human avatar from a single image. | Training: 16 Nvidia A100 80G GPUs. Inference: A single RTX-3090 GPU. | The optimization process to create an animatable avatar is not real-time and requires several minutes. Future work aims to explore feed-forward reconstruction techniques that are robust to multi-view inconsistencies. |  
|**(CVPR25)GASP: Gaussian Avatars with Synthetic Priors**|  | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2412.07739)<br>  Not Release code |
|**(CVPR25)GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view Diffusion**| A short, monocular video sequence (e.g., 10-15 seconds) captured by a commodity device like a smartphone| Head pose and facial expressions are controllable via the underlying parametric FLAME model. | Rendering is real-time. The method achieves 62 FPS for rendering an image at 802x550 resolution. However, the initial avatar reconstruction/optimization process is offline and time-consuming (approx. 12 hours). | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2412.10209)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/tangjiapeng/GAF) not release code| 1.  A novel approach to reconstruct animatable head avatars from monocular videos by using multi-view head diffusion priors to regularize and complete unobserved regions. 2. A multi-view head diffusion model guided by normal maps rendered from FLAME reconstructions, enabling more precise viewpoint control for generating consistent multi-view images. 3. A strategy to enhance photorealism by using iteratively denoised images (as pseudo-ground truths) and integrating a latent upsampler to refine facial details. | Monocular. The method is specifically designed to work with single-camera videos that have limited viewpoints. | Reconstructing photorealistic 3D head avatars from monocular videos is challenging because limited observations leave unobserved regions (e.g., the sides and back of the head) under-constrained, which can lead to significant artifacts when rendering novel viewsÔΩú multi-view head diffusion model was trained on eight A100 GPUs
|**(CVPR25)Creating Your Editable 3D Photorealistic Avatar with Tetrahedron-constrained Gaussian Splatting**| A short (40-50 seconds) monocular RGB video of a person, plus a user-provided text prompt or a reference garment image. | Localized geometry and appearance of the 3D avatar, primarily for editing or trying on different garments  | Training per edit: ‚Ä¢ Localized spatial adaptation (geometry): ~1.2 hours. ‚Ä¢ Texture inpainting (coarse appearance): 20 minutes. ‚Ä¢ Attribute activation (fine appearance): 3 minutes. | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2504.20403)<br>  Not Release code | 1.  A novel system for creating photorealistic, editable 3D avatars from monocular videos using user prompts . 2. TetGS, a novel hybrid representation that combines the controllable structure of tetrahedral grids with the high-fidelity rendering of 3D Gaussian Splatting . 3.  A decoupled editing pipeline that separates the process into localized spatial (geometry) adaptation and appearance generation, enabling stable and high-quality results | Input is a monocular video, but the output is a full 360¬∞ editable 3D avatar. | Addressing the instability and poor quality of the previous 3D avatar editing methods. The goal is to enable precise, localized editing with both geometric adaptability and photorealistic appearance. |  single NVIDIA A40 GPU | 1.  Static Scenes Only: It cannot handle dynamic motion. 2.  Extreme Editing Cases: It may struggle to generate proper geometry when editing from very loose garments (e.g., a dress) to tight-fitting ones. |
| **(CVPR25)RigGS: Rigging of 3D Gaussians for Modeling Articulated Objects in Videos** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2503.16822)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/yaoyx689/RigGS)| | |
|**(CVPR25 Highlight)FRESA: Feedforward Reconstruction of Personalized Skinned Avatars from Few Images**|  | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2503.19207)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/rongakowang/FRESA)Not Release code |
|**(CVPR25 Highlight)Real-time High-fidelity Gaussian Human Avatars with Position-based Interpolation of Spatially Distributed MLPs**| Multi-view videos of a person, along with extracted foreground masks and registered SMPL-X models for 3D pose information | The pose of the human avatar. The model takes a novel pose vector as input to animate the avatar and render it from new viewpoints | Rendering: Real-time at 166 FPS. Training: 17.5 hours for 800K iterations | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2504.12909)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/1231234zhan/mmlphuman) | 1. Spatially Distributed MLPs: A novel architecture where many small MLPs, which only take pose as input, are distributed on the body surface. This drastically reduces computation and enables real-time rendering. 2. Gaussian Offset Basis: A mechanism to learn high-frequency details (e.g., wrinkles, text). It combines smoothly interpolated coefficients with freely learned basis vectors, overcoming the smoothing artifacts of direct interpolation. 3. Control Points: A system to constrain Gaussians to a surface layer by interpolating their position offsets from control points. This improves generalization to novel poses and eliminates artifacts| Multi-view video capture is required. Experiments show it can work with as few as 3 views, up to 39 views for higher quality. | Prior methods for creating animatable Gaussian avatars struggled to balance quality and performance. They were either fast but lacked fine, pose-dependent details, or were high-fidelity but too computationally intensive for real-time rendering (e.g., ~10 FPS). | All evaluations were performed on a single NVIDIA 3090 GPU | 1.  Appearance is solely conditioned on pose, meaning it cannot model complex, non-pose-related cloth dynamics like swaying in the wind.2.  The pipeline relies on a heavy pre-processing setup, including multi-view capture, pose estimation, and template mesh extraction.3.  It currently cannot reconstruct avatars from monocular (single-view) videos|
|**(CVPR25)Vid2Avatar-Pro: Authentic Avatar from Videos in the Wild via Universal Prior** Vid2Avatar(CVPR2023)| A single, monocular "in-the-wild" video of a person | novel human motions (body poses) and rendered from arbitrary novel viewpoints.  |  | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://moygcc.github.io/vid2avatar-pro/static/CVPR2025_Vid2Avatar_Pro.pdf)<br>  Not Release code | 1. A universal prior model for clothed humans learned from a large-scale (1000 identities) high-quality dynamic performance capture dataset.  2. A novel universal prior model architecture using spatially normalized front and back maps as identity conditioning, enabling scalable training across multiple identities. 3. A robust personalization pipeline to create state-of-the-art, photorealistic, and animatable avatars from monocular videos. | Monocular | Addresses the challenges of creating high-quality, animatable 3D avatars from monocular videos where pose diversity and viewpoints are limited. |  | 1. Lack of facial animation. 2. Loose clothing: The model struggles with subjects dressed in very loose garments due to a lack of such data.  3. Lighting conditions: may not perform well under extreme lighting variations. |
|**(CVPR25)WildAvatar: Learning In-the-wild 3D Avatars from the Web**| | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2407.02165)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/wildavatar/WildAvatar_Toolbox) |
|**(CVPR25)IDOL: Instant Photorealistic 3D Human Creation from a Single Image**| A single high-resolution (1024√ó1024) RGB image of a human. | Texture/Appearance: Can be modified by editing the generated UV texture maps. Body Shape: SMPL-X shape parameters. Pose: Animatable (SMPL-X parameters) |  reconstructs a 3D human in under 1 second  | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2412.14963)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/yiyuzhuang/IDOL) | 1. HuGe100K Dataset: A new large-scale (100K subjects, 2.4M+ images) multi-view human dataset with diverse attributes, generated via a scalable pipeline.  2. IDOL Model: An efficient, feed-forward transformer-based model for instant, high-fidelity 3D human reconstruction from a single image. 3. Scalable Framework: Demonstrates that leveraging large-scale generated data significantly enhances model performance and generalizability in 3D human reconstruction. | Input: Single-view. Training Data: Multi-view (24 uniformly sampled views covering a 360¬∞ rotation for each subject). Output: A full 3D model that allows for novel-view synthesis from any viewpoint. | | Training: Performed on a cluster of 32 NVIDIA H100 GPUs for approximately one day. Inference: Performed on a single NVIDIA A100 GPU | 1. Motion Generation: The data generation pipeline is limited to synthesizing single-frame images from fixed viewpoints, not continuous, long motion sequences. 2. Facial Detail: Architecture lacks specific designs for these aspects. 3. Partial Inputs: Handling half-body or other partial inputs remains challenging. |
|**(ICCV25)Sequential Gaussian Avatars with Hierarchical Motion Context**| Multi-view RGB video sequences with corresponding camera parameters and pre-processed SMPL(-X) poses. | avatar can be animated and rendered in novel poses | Training: Fast, e.g., ~25 minutes on ZJU-MoCap dataset.  Rendering: Real-time, ~45 FPS on the I3D-Human dataset, and ~25 FPS on ZJU-MoCap.| [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2411.16768)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/zezeaaa/SeqAvatar) | 1.  Proposed a novel hierarchical motion condition that combines coarse-to-fine human motion (global skeleton poses and localized vertex residuals) to enhance non-rigid deformation prediction. 2. Introduced a spatiotemporal multi-scale sampling strategy to expand the receptive field of the motion context, improving generalization to complex motions. 3. Achieved state-of-the-art rendering performance on challenging datasets with complex motions and garments.| Multi-view. | SMPL-driven 3DGS human avatars struggle to capture fine appearance details (e.g., non-rigid deformation of loose garments) because the mapping from a single static pose to appearance is complex and ambiguous, and these methods often lack temporal motion context. | A single NVIDIA RTX 4090 GPU | 1.  The Gaussian-based representation may introduce slight blur artifacts compared to NeRF's sharper ray-based rendering. 2. The local velocity cues are derived from the coarse SMPL model rather than dense surface tracking, which may limit the accuracy of fine-scale garment deformation modeling.|
|**(ICCV25)GAS: Generative Avatar Synthesis from a Single Image**| A single reference image of a person | novel camera views and novel human poses | 0.40 fps on an NVIDIA A800 GPU. | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2502.06957)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/humansensinglab/GAS) Not Release Full Code | 1. A unified framework for both novel view and pose synthesis that enables training on large-scale, in-the-wild data for better generalization. 2. The use of a dense appearance cue from generalizable NeRF renderings as guidance for the diffusion model, ensuring consistent appearance across views and poses. | novel camera views and novel human poses | Addressing the multi-view and temporal inconsistencies in single-image avatar generation, which arise from the mismatch between sparse conditioning signals (like depth maps) used in prior methods and the subject's true appearance. | Training: 8x A100 GPUs . Inference: A single NVIDIA A800 GPU. |   1. Lacks expressiveness in detailed areas like the face and hands, which can lead to artifacts. 2. The model sometimes struggles to accurately generate complex clothing textures. |
|**(ICCV25 Hightlight)MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar Reconstruction**| A single in-the-wild RGB image. | pose of the avatar | Not specified | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://zj-dong.github.io/MoGA/assets/paper.pdf)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/zj-dong/MoGA) Not Release Code | 1. An optimization-based framework that reconstructs a Gaussian avatar from a single image by fitting a 3D generative model to synthetic views generated by 2D multi-view diffusion. 2. A generative 3D Gaussian avatar prior that provides meaningful initialization, strong 3D regularization, and support for pose refinement during the fitting process. 3. The method generalizes well to in-the-wild images with challenging poses and clothing, and the resulting avatars are animatable without post-processing.| Input: Monocular. Output: Allows for novel view synthesis | Reconstructing high-fidelity, animatable 3D Gaussian avatars from a single image. | Not specified ||
|**(ICCV25)ToMiE: Towards Explicit Exoskeleton for the Reconstruction of Complicated 3D Human Avatars**| Multi-view synchronized videos.|The poses of hand-held objects and the motion of loose-fitting clothing. This control is explicit and fully decoupled from the standard SMPL body pose, allowing for independent animation |Training Time: ~30 minutes. Rendering Speed (FPS): 60+ FPS.| [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2410.08082)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/Yifever20002/ToMiE?tab=readme-ov-file) | 1. Proposes ToMiE, an adaptive growth strategy to create an enhanced SMPL joint tree, decoupling complex parts from the body for state-of-the-art rendering and animation. 2. Introduces a hybrid assignment strategy using LBS weights and Motion Kernels to guide the growth of external joints via gradient localization. 3. Develops a joint optimization approach that fits local rotations across frames for the newly grown joints |  Multi-view synchronized videos. 24 surrounding views for training and 6 novel views for testing | Standard parameterized models like SMPL fail to accurately model and animate 3D humans with hand-held objects or loose-fitting clothing | single GeForce RTX3090 GPU | drastic topological changes, such as a person taking off their clothes or opening a book.|
|**(ICCV25)PERSONA: Personalized Whole-Body 3D Avatar with Pose-Driven Deformations from a Single Image**|A single image|Novel whole-body poses and facial expressions|Yes (for rendering) / 25.56 FPS. (Note: Pre-processing/training is offline: ~1h video gen + 30min training )| [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2508.09973)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/mks0601/PERSONA_RELEASE) |<ul><li>A framework (PERSONA) to create personalized 3D avatars with pose-driven deformations from a single image .</li><li>Uses diffusion-generated videos for training, removing the need for extensive per-individual video capture .</li><li>Balanced Sampling: A technique to ensure authentic identity consistency by oversampling the input image and mitigating baked-in artifacts .</li><li>Geometry-weighted Optimization: A method that prioritizes geometry constraints over image loss to maintain sharp renderings across diverse poses.</li></ul> | Input: Single-view. Output: 3D avatar renderable from novel (any) viewpoints.| 
|**(ICCV25)Fine-Grained 3D Gaussian Head Avatars Modeling from Static Captures via Joint Reconstruction and Registration**|  | | | Not release paper and codes |
|**(ICCV25)Disentangled Clothed Avatar Generation with Layered Representation**| Gaussian noise (for generation). The model is trained on multi-view 2D images | Animation; Camera View Control; Decomposition of body, hair, and clothes; Component Transfer (e.g., swapping clothes, hair, shoes) | Seconds to generate a single avatar | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2501.04631)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://arxiv.org/pdf/2501.04631) | 1. Proposes LayerAvatar, a novel feed-forward diffusion-based pipeline for generating component-disentangled clothed avatars. 2. Introduces a layered UV feature plane representation that facilitates component disentanglement and enhances generation quality. 3. Achieves state-of-the-art performance in generation quality and supports downstream applications like component transfer with high efficiency | Multi-view 2D images are used for training | The challenge is efficiently generating high-quality, customizable 3D clothed avatars. | Training: The model was trained for 6 days on two RTX 3090 GPUs | 1. Performance is sensitive to the accuracy of the estimated segmentation maps and SMPL-X parameters. 2. Potential for collision artifacts between the body and clothing layers. 3. Animation of loose clothing is prone to artifacts. 4. It does not currently handle general accessories like glasses or bags |
|**(ICCV25)LHM: Large Animatable Human Reconstruction Model for Single Image to 3D in Seconds**| A single RGB image of a person | controllable for animation via SMPL-X parameters | 2.01s (LHM-0.5B), 4.13s (LHM-0.7B), and 6.57s (LHM-1B). | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2503.10625)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/aigc3d/LHM) | 1. Proposes LHM, a generalizable feed-forward model that reconstructs animatable 3D avatars from a single image in seconds. 2. Introduces a Multimodal Body-Head Transformer (MBHT) to effectively fuse 3D geometric tokens with 2D image features for detailed reconstruction. 3. Achieves state-of-the-art performance in generalization and animation consistency by training on a large-scale video dataset without requiring rigged 3D ground truth data. | single-view image | challenging problem of animatable 3D human reconstruction from a single image | Training: Performed on NVIDIA A100 clusters using 32 to 64 GPUs. Inference: from 18 GB to 24 GB, depending on the model size. | can be affected by the biased distribution of views and poses in the real-world video training dataset |
| **(ICCV25)Relightable Full-Body Gaussian Codec Avatars** |3D body and face keypoints; Lighting conditions (environment maps / point lights); Viewing direction.|Avatar pose (animation); Environmental lighting (relighting); Viewpoint.|The paper does not explicitly state the rendering FPS. Training takes approximately 2 days on a single A100 GPU| [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2501.14726)<br>  Not Release Code |1. The first approach for relightable, high-fidelity, and animatable full-body avatars (jointly modeling body, face, and hands). 2. Proposes learnable Zonal Harmonics (ZH) to represent diffuse light transport, which efficiently handles body articulation. 3. Introduces a dedicated shadow network with physically based irradiance normalization to model non-local shadows caused by articulation. 4. Uses deferred shading for specular radiance transfer to achieve high-fidelity facial reflections (e.g., eye glints) without an excessive number of Gaussians.|Trained on data from a multi-camera light stage (512 cameras). It supports free-viewpoint rendering and is evaluated on novel views.|
| **(ICCV25)TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented Reality via 3D Gaussian Splatting** |||| [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/abs/2503.17032)<br>  Not Release Code ||multi-view||||
| **(ICCV25)EVA: Expressive Virtual Avatars from Multi-view Videos** |1. Skeletal Motion ( $\theta^{\circ}$ ) 2. Facial Expressions ( $\psi$ ) 3. Virtual Viewpoint ( $V$ ) |Disentangled (independent) control over: 1. Body movements  2. Hand gestures  3. Facial expressions|Real-time:  ‚Ä¢ Geometry layer: $\approx41$ FPS ‚Ä¢ Appearance layer: $\approx35$ FPS ‚Ä¢ Renders 2K images in real-time| [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2505.15385)<br>  Not Release Code |1. EVA Framework: A novel actor-specific avatar method that is real-time, photo-realistic, and offers fully disentangled control. 2. Expressive Deformable Template: A hybrid geometric proxy combining a motion-driven deformable body (for clothing dynamics) with a personalized parametric head (for facial expressions). 3. Disentangled Gaussian Appearance: An appearance model using two separate modules (U-Nets) to independently predict 3D Gaussian parameters for the body and face in UV space, enabling decoupled control.|Dense Multi-view setup for training$\approx100$ cameras for full-body capture||Template Fitting: NVIDIA GeForce RTX 3090  ‚Ä¢ Model Training: NVIDIA A100/A40 GPUs (DDC part used 4 GPUs; Appearance part used 1 GPU ) ‚Ä¢ Inference (Real-time): Two A40 GPUs (one for geometry, one for appearance)|1. Topological Changes: it cannot handle topological changes like removing clothing. 2. Fixed Gaussian Count: The number of Gaussians is fixed, limiting multi-scale rendering. 3. Seam Artifacts: Potential for "minor visual inconsistencies at the neck" where the head and body models are stitched. 4. Lighting: Lighting is not physically-based; it's learned from studio conditions and constrained by them. 5. Cross-region Effects: Cannot model cross-region lighting or shadows (e.g., a hand casting a shadow on the face).|
| **(NeurIPS25)MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust Physics-Based Dynamics** | | | |[![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b?style=for-the-badge.svg)](https://arxiv.org/pdf/2510.01619)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2?style=for-the-badge.svg)](https://github.com/KAISTChangmin/MPMAvatar)| | | |
| **(Arxiv25)Dream3DAvatar: Text-Controlled 3D Avatar Reconstruction from a Single Image** | | | |[![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b?style=for-the-badge.svg)](https://arxiv.org/pdf/2509.13013)<br>  Not Release Code| | | |
|**(ICCV25)One Shot, One Talk: Whole-body Talking Avatar from a Single Image**| | | |[![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b?style=for-the-badge.svg)](https://arxiv.org/pdf/2412.01106)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2?style=for-the-badge.svg)](https://ustc3dv.github.io/OneShotOneTalk/)Not Release Code|
|**(TVCG25)WonderHuman: Hallucinating Unseen Parts in Dynamic 3D Human Reconstruction**|Monocular video, specifically partial-view or front-view videos|Pose.|Training (Total): ~4.5 hours (including ~40 min preprocessing, ~40 min Stage I, ~20 min post-processing, and ~3 hours Stage II). Inference: 13-18 frames per second (FPS).|[![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b?style=for-the-badge.svg)](https://arxiv.org/pdf/2502.01045)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2?style=for-the-badge.svg)](https://github.com/wyiguanw/WonderHuman)Not Release Full Code|1. WonderHuman Framework: A novel framework using 2D generative diffusion priors to reconstruct high-quality dynamic humans from monocular videos, including unseen parts. 2. Dual-space Optimization: Applies Score Distillation Sampling (SDS) in both canonical and observation spaces to ensure visual consistency and realism. 3. View Selection & Pose Feature Injection: Strategies to resolve conflicts between SDS predictions and observed data, ensuring pose-dependent effects and higher fidelity. ÔΩúMonocularand Partial-view / Limited-view ÔΩúReconstructing high-fidelity, dynamic 3D human avatars from monocular videos that have limited viewpoints (e.g., only front-view). Previous methods typically fail in this setting because they cannot reconstruct the unseen body parts (e.g., the back) and require full-view coverage. ÔΩú Stage I Training: Single RTX-3090 GPU. Stage II Training: Two RTX-3090 GPUs ÔΩú
|**(CVPR25 Workshop)SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with Video Diffusion and Data Augmentation**|Core Method: A single RGB image. Applications: Text prompts (for Text-to-3D Avatar)or an Input Image + Text Prompt (for Text-Guided Editing)|Pose.|Generation/Training: 5-6 hours total per avatar. Breakdown:* 3DGS Avatar Training: ~273 min * SMPL-X Fitting: ~61 min. Video Diffusion: ~18 min * Image Restoration: ~11 min. Rendering: Real-time|[![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b?style=for-the-badge.svg)](https://arxiv.org/pdf/2505.05475)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2?style=for-the-badge.svg)](https://github.com/yc4ny/SVAD)|1. A novel pipeline to create high-fidelity, animatable 3D Gaussian Splatting (3DGS) avatars from a single image.  2. The core idea of generating synthetic training data for the 3DGS avatar using a video diffusion model. 3. A Data Augmentation Module (with Identity Preservation and Image Restoration) to refine the synthetic data, ensuring high fidelity and identity consistency across frames. ÔΩúInput: Single-view. Internal: The method synthetically generates a multi-view video | To create high-quality, animatable 3D avatars from a single image. It aims to solve the key limitations of existing methods:1. 3DGS methods require multi-view or video inputs, which aren't available.  2. Video diffusion models can animate from a single image but suffer from temporal inconsistency and identity drift. ÔΩú NVIDIA A100-SXM4-80GB GPU ÔΩú
|**(Arxiv25)PF-LHM: 3D Animatable Avatar Reconstruction from Pose-free Articulated Human Images**|N >= 1(one or multiple) casually captured RGB images of a human subject. Crucially: Does not require camera parameters or human pose annotations|The reconstructed avatar is animatable|In seconds|[![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b?style=for-the-badge.svg)](https://arxiv.org/pdf/2505.05475)<br>  Not Release Code|1.  The first feed-forward model to reconstruct high-quality, animatable 3D human avatars in seconds from $\ge 1$ casually captured, pose-free and camera-free images.2.  A novel Encoder-Decoder Point-Image Transformer (PIT) architecture for efficient, hierarchical fusion of 3D geometric features and 2D multi-image features using multimodal attention.3.  Unifies single- and multi-image reconstruction, achieving superior generalization and visual quality| | |Training: Performed on 32 A100 GPUs over five days.  Inference (Testing): Evaluated on NVIDIA A100-80G hardware|








---


### Pose & Expression Control

| Title | Focus | Controlled Variables | Link | Modality | Contribution | Views Type |
|---|---|---|---|---|---|---|
| SEGA | head / face; disentangle dynamic vs static parts; control expression + view + identity | Expression, view, identity | [arXiv](https://arxiv.org/abs/2504.14373) :contentReference[oaicite:3]{index=3} |
| GAvatar | full‚Äêbody / identity; geometry + appearance; animatable via pose | Pose, identity, geometry detail | [arXiv](https://arxiv.org/abs/2312.11461) :contentReference[oaicite:4]{index=4} |

---


### 3D/4D Generation with Gaussian Splatting

| Title | Model Input | What is Controllable | Speed | Papers & Codes | Contribution | Views Type | specified problem | Training & Inference Device | Limitations |
|---|---|---|---|---|---|---|---|---|---|
| **(ICLR24 Oral)DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation** | * Single-view image (for Image-to-3D)  * Text prompt (for Text-to-3D). | 3D geometry and appearance (controlled via the input text or image) | * Image-to-3D: ~2 minutes to produce a high-quality textured mesh. * Text-to-3D: ~5 minutes (using 2 stages, each ~2 mins ). |[![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b?style=for-the-badge.svg)](https://arxiv.org/pdf/2309.16653)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2?style=for-the-badge.svg)](https://github.com/dreamgaussian/dreamgaussian)| 1. Adapted 3D Gaussian Splatting (3DGS) for efficient optimization-based 3D generation (2D lifting), demonstrating it converges much faster than NeRF for generative tasks. 2. Proposed an efficient algorithm to extract a textured mesh from 3D Gaussians using local density queries and Marching Cubes. 3. Designed a stable UV-space texture refinement stage that uses an MSE loss (guided by multi-step denoising, inspired by SDEdit) to avoid SDS artifacts. 4. Achieved significant acceleration (~10x) compared to prior optimization methods, generating textured meshes from images in 2 minutes. | * Input: Single-view image or Text (no view). * Optimization: Uses random camera poses orbiting the object during SDS optimization. | Addresses the "slow per-sample optimization" (often hours-long) of existing 2D lifting methods that typically use NeRF and SDS for 3D generation. |
| **(CVPR24)GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models** |  | | |[![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b?style=for-the-badge.svg)](https://arxiv.org/pdf/2310.08529)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2?style=for-the-badge.svg)](https://github.com/hustvl/GaussianDreamer)| | | |
| **(CVPR24)HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting** |  | | |[![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b?style=for-the-badge.svg)](https://arxiv.org/abs/2311.17061)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2?style=for-the-badge.svg)](https://github.com/alvinliu0/HumanGaussian)| | | |
| **(CVPR24 Highlight)LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching** |  | | |[![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b?style=for-the-badge.svg)](https://arxiv.org/pdf/2311.11284)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2?style=for-the-badge.svg)](https://github.com/EnVision-Research/LucidDreamer)| | | |
| **(arxiv 23)DreamGaussian4D: Generative 4D Gaussian Splatting** |Primarily a single image It can also be adapted for Video-to-4D, using a monocular video|The 3D motion of the generated 4D asset|Image-to-4D:* ~6.5 minutes for 4D GS generation (Stage 1).* ~10 minutes total with optional Video-to-Video texture refinement (Stage 2).Video-to-4D:5-15 minutes depending on the configuration.|[![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b?style=for-the-badge.svg)](https://arxiv.org/abs/2312.17142)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2?style=for-the-badge.svg)](https://github.com/jiawei-ren/dreamgaussian4d?tab=readme-ov-file)|1. Principled Image-to-4D Framework: A novel framework that combines image-conditioned 3D generation and video generation models. 2. Explicit 4D Representation: Uses static 3D GS plus a HexPlane-based deformation field, drastically cutting 4D generation time from hours to minutes. 3. V2V Texture Refinement: A new Video-to-Video pipeline to refine mesh textures, enhancing quality while maintaining temporal consistency. 4. Superior Performance: Achieves high-quality 4D generation and motion control with significantly shorter optimization time| | |All experiments were run on a single 80 GB A100 GPUÔΩú
| **(CVPR24)4D-fy: Text-to-4D Generation Using Hybrid Score Distillation Sampling** |  | | |[![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b?style=for-the-badge.svg)](https://arxiv.org/pdf/2311.17984)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2?style=for-the-badge.svg)](https://sherwinbahmani.github.io/4dfy/)| | | |
| **(ECCV24)Learn to Optimize Denoising Scores for 3D Generation: A Unified and Improved Diffusion Prior on NeRF and 3D Gaussian Splatting** |  | | |[![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b?style=for-the-badge.svg)](https://arxiv.org/pdf/2311.17984)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2?style=for-the-badge.svg)](https://github.com/yangxiaofeng/LODS)| | | |
| **(CVPR24)Dream-in-4D: A Unified Approach for Text- and Image-guided 4D Scene Generation** |  | | |[![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b?style=for-the-badge.svg)](https://arxiv.org/pdf/2311.16854)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2?style=for-the-badge.svg)](https://github.com/NVlabs/dream-in-4d)| | | |* Stage 1 Training: NVIDIA V100 GPU.* Stage 2 Training: NVIDIA A100 or RTX A6000 GPU.ÔΩú



---






## üîßImplementations & Code

- Whenever available, include GitHub / project URLs  
- Example: if SEGA / GAvatar have official code releases, include here (you may need to check if code is published yet)  
- For projects without code, note that status

---

## üìÇDatasets

- List datasets used by the papers above (e.g. datasets with multi‚Äêview images, expression / pose annotations, identity variation)  
- If there are avatar / human scan datasets useful for 3DGS avatar work, include them

---

## üé•Demos & Videos

- Link to project demo pages  
- Embeddable video / gif if available  
- Screenshots of avatars / animations help

---

## üìñSurveys & Related Resources

- *A Survey on 3D Human Avatar Modeling -- From Reconstruction to Generation* ‚Äî Links: [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2406.04253)
 
---

## üìãScope / What‚Äôs Included

This repository collects methods and resources that satisfy **all** of:

1. Use **3D Gaussian Splatting** (or methods clearly based on or extending 3DGS) as a core component.  
2. Target **avatars / dynamic humans** (head, face, full body, clothed, expression / pose animation).  
3. Support or demonstrate **dynamic / controllable outputs** (pose, expression, identity, view etc.).  

Excluded (for now):

- Static scenes / static objects without human / avatar elements.  
- Methods that use only 2D Gaussian Splatting without 3D extension.  
- Non-human characters unless specified.  

---

## ü§ùContributing

We welcome contributions! If you find a new paper / code repo / demo / dataset relevant to **3D Gaussian Splatting + avatars / dynamic humans**, please send a Pull Request. When submitting, please include:

- Title, authors, year  
- Type of avatar (head / full body / clothed)  
- Input modality (single image / video / multi-view / text prompt etc.)  
- Controlled variables (pose, expression, identity, view etc.)  
- Speed / resource info (if available)  
- Code / demo / project page link 
---

## ‚ö†Ô∏èLicense

This repository is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

---

## üôèAcknowledgments

Thank you to the authors of all the papers/projects listed, and to the wider 3DGS community.  

---

