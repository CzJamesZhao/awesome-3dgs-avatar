# awesome-3dgs-avatar
**Awesome 3D Gaussian Splatting ‚Üí Avatars & Dynamic Humans**

A curated list of papers, implementations, datasets, demos, and resources focusing on **3D Gaussian Splatting (3DGS)** methods applied to **avatars / dynamic human modeling**: head avatars, full-body clothed avatars, expression & pose control, single-image / video / multi-view inputs, and real-time rendering.

‚≠ê If you like this list, please give it a star! üòÑ


[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](#license)  
[![Last Updated](https://img.shields.io/badge/last%20updated-2025--10--18-blue.svg)](#)  
[![Contributions Welcome](https://img.shields.io/badge/Contributions-Welcome-green.svg)](#contributing)  


---

## üöÄ Table of Contents

- [Papers](#papers)
  - [Head and Face Avatars](#head--face-avatars)
  - [Full-Body and Clothed Avatars](#full--upper--clothed-avatars)
  - [Pose and Expression Control](#pose--expression-control)
- [Implementations and Code](#implementations--code)
- [Datasets](#datasets)
- [Demos and Videos](#demos--videos)
- [Surveys and Related Resources](#surveys--related-resources)
- [Scope and What's Included](#scope--whats-included)
- [Contributing](#contributing)
- [License](#license)

---

## üìöPapers

### Head & Face Avatars

| Title | Model Input | What is Controllable | Real-Time? / Speed | Papers & Codes | Contribution | Views Type |
|---|---|---|---|---|---|---|
| **(NeurIPS24)Generalizable and Animatable Gaussian Head Avatar** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2410.07971)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/xg-chu/GAGAvatar)  | | |
| **(CVPR24)Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2312.03029)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/YuelangX/Gaussian-Head-Avatar)  | | |
| **(25)SEGA: Drivable 3D Gaussian Head Avatar from a Single Image** | Single image; using UV-space Gaussian framework + FLAME prior | Expression + view + identity; generalizes to unseen identities | Real-time performance claimed | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/abs/2504.14373) Not release code|
| **(CVPR25)3D Gaussian Head Avatars with Expressive Dynamic Appearances by Compact Tensorial Representations** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/abs/2504.14967)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/ant-research/TensorialGaussianAvatar)  | | |
| **(CVPR25)Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID Guidance** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/abs/2501.05379)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/dimgerogiannis/Arc2Avatar)  | | |
| **(CVPR25)AvatarArtist: Open-Domain 4D Avatarization** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2503.19906)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/ant-research/AvatarArtist)  | | |
| **(CVPR25 Oral)CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/abs/2412.12093)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/felixtaubner/cap4d/)  | | |
| **(CVPR25)FATE: Full-head Gaussian Avatar with Textural Editing from Monocular Video** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/abs/2411.15604)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/zjwfufu/FateAvatar)  | | |
| **(CVPR25)GPAvatar: High-fidelity Head Avatars by Learning Efficient Gaussian Projections** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://openaccess.thecvf.com/content/CVPR2025/papers/Feng_GPAvatar_High-fidelity_Head_Avatars_by_Learning_Efficient_Gaussian_Projections_CVPR_2025_paper.pdf)<br>  Not Release Code  | | |
| **(CVPR25)HERA: Hybrid Explicit Representation for Ultra-Realistic Head Avatars** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2403.11453)<br>  Not Release Code  | | |
| **(CVPR25)HRAvatar: High-Quality and Relightable Gaussian Head Avatar** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2503.08224)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/Pixel-Talk/HRAvatar)  | | |
| **(CVPR25)LUCAS: Layered Universal Codec Avatars** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2502.19739)<br>  Not Release Code  | | |
| **(CVPR25)MeGA: Hybrid Mesh-Gaussian Head Avatar for High-Fidelity Rendering and Head Editing** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2404.19026)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/conallwang/MeGA)  | | |
| **(CVPR25)MobilePortrait: Real-Time One-Shot Neural Head Avatars on Mobile Devices** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2407.05712)<br>  Not Release Code  | | |
| **(CVPR25)PERSE: Personalized 3D Generative Avatars from A Single Portrait** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2412.21206)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/snuvclab/perse?tab=readme-ov-file)  | | |
| **(CVPR25)RGBAvatar: Reduced Gaussian Blendshapes for Online Modeling of Head Avatars** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2503.12886)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/gapszju/RGBAvatar)  | | |
| **(CVPR25)Synthetic Prior for Few-Shot Drivable Head Avatar Inversion** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2501.06903)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/Zielon/synshot) Not Release Code | | |
| **(CVPR25)Synthetic Prior for Few-Shot Drivable Head Avatar Inversion** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2503.00495)<br>  Not Release Code | | |
| **(NeurIPS25)Low-Rank Head Avatar Personalization with Registers** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2506.01935)<br>  Not Release Code | | |
| **(NeurIPS25)BecomingLit: Relightable Gaussian Avatars with Hybrid Neural Shading** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2506.06271)<br>  Not Release Code | | |
| **(ICCV25)FaceCraft4D: Animated 3D Facial Avatar Generation from a Single Image** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2504.15179)<br>  Not Release Code | | |
| **(ICCV25)StrandHead: Text to Hair-Disentangled 3D Head Avatars Using Human-Centric Priors** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://github.com/XiaokunSun/StrandHead)<br>  Not Release Code | | |
| **(ICCV25)TeRA : Rethinking Text-guided Realistic 3D Avatar Generation** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://zjwsite.github.io/files/TeRA.pdf)<br>  Not Release Code | | |
| **(ICCV25)Large Animatable Gaussian Reconstruction Model for High-fidelity 3D Head Avatars** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://tobias-kirschstein.github.io/avat3r/)<br>  Not Release Code | | |
| **(ICCV25)InteractAvatar: Modeling Hand-Face Interaction in Photorealistic Avatars with Deformable Gaussians** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2504.07949)<br>  Not Release Code | | |
| **(ICCV25)GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar** | | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2507.18155)<br>  Not Release Code | | |








---

### Full / Upper / Clothed Avatars

| Title | Model Input | What is Controllable | Speed | Papers & Codes | Contribution | Views Type | specified problem | Training & Inference Device | Limitations |
|---|---|---|---|---|---|---|---|---|---|
| **(MM24)Animatable 3D Gaussian: Fast and High-Quality Reconstruction of Multiple Human Avatars** | video sequence of one or more humans along with their corresponding pose data $(S_t,T_t)$ for each frame. | novel view synthesis and novel pose synthesis | real-time claimed |[![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b?style=for-the-badge.svg)](https://arxiv.org/pdf/2311.16482)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2?style=for-the-badge.svg)](https://github.com/jimmyYliu/Animatable-3d-Gaussian)| 1. Animatable 3D Gaussian: A new representation for dynamic humans that allows for fast, high-quality reconstruction without needing a specific shape prior like SMPL. 2. significantly faster (training in seconds) and uses less memory than previous state-of-the-art methods. 3. Dynamic Shadow Modeling. | monocular (single-view) or sparse multi-view video sequences |
| **(3DV25)D3GA - Drivable 3D Gaussian Avatars** | 3D joint angles and facial keypoints | body, individual garments (like shirts and pants), and the face. | 1024√ó667 resolution<br> it achieves approximately 26 FPS | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2311.08581)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/facebookresearch/D3GA) | 1. A lightweight and composable avatar model using 3D Gaussians that are deformed by tetrahedral cages instead of standard Linear Blend Skinning (LBS). This allows for more natural stretching and re-orientation of the primitives. 2. The ability to use localized conditioning, meaning different parts of the avatar (e.g., face, body) can be driven by different input signals (e.g., keypoints, joint angles). | dense multi-view video captured in a studio setting. 200 cameras as well as a smaller 40-camera setup |
| **(23)Human101: Training 100+FPS Human Gaussians in 100s from 1 View** | A single-view video, pre-calibrated camera parameters, SMPL (human pose and shape) parameters for each frame, and foreground masks. | SMPL pose (Œ∏) and shape (Œ≤)  | Rendering is real-time | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2312.15258.pdf)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/longxiang-ai/Human101) Not release code| 1.  Pioneers the use of 3D Gaussian Splatting (3D GS) for dynamic human reconstruction, leveraging its explicit representation for efficient rendering. 2.  Proposes a Canonical Human Initialization method that creates a high-quality initial model by fusing point clouds, which significantly accelerates convergence. 3.  Introduces a Human-centric Forward Gaussian Animation method that is more efficient than the traditional backward skinning used in NeRF-based approaches, enabling fast pose-driven animation| Monocular |
| **(CVPR24)SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes** | An image sequence from a monocular dynamic video | Scene motion can be edited by manipulating a graph of sparse control points. | high rendering speed | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://yihua7.github.io/SC-GS-web/materials/SC_GS_Arxiv.pdf)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/CVMI-Lab/SC-GS) | 1. Proposes a novel representation that uses sparse control points and an MLP to model the motion of dense 3D Gaussians for dynamic scenes.  2. Introduces an adaptive strategy to adjust the number of control points and an "As-Rigid-As-Possible" (ARAP) loss to ensure plausible and smooth motions.  3. Enables user-controlled motion editing by deforming a control point graph while preserving high-fidelity appearance | Takes monocular video as input to synthesize novel (free-viewpoint) views of the dynamic scene. |
| **(CVPR24)GauHuman: Articulated Gaussian Splatting from Monocular Human Videos** | Monocular RGB Video (single view video) , along with pre-processed camera parameters, foreground masks, and SMPL parameters. | novel poses for that subject's avatar by providing new SMPL pose parameters. | real-time rendering.<br> Training Speed: around 1-2 minutes on a single GPU. | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2312.02973.pdf)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/skhu101/GauHuman) | 1. Proposes a novel articulated Gaussian Splatting framework for 3D human modeling that achieves both fast training and real-time rendering. 2. Introduces effective pose and LBS (Linear Blend Skinning) refinement modules to capture fine details. 3. Designs a fast optimization strategy using human priors (SMPL) for initialization/pruning and KL-divergence guidance for splitting/cloning Gaussians, plus a new merge operation | Monocular | efficiency bottleneck in creating high-quality 3D human avatars from monocular videos | single NVIDIA RTX 3090 | 1) The current framework is composed of discrete Gaussian spheres, making it difficult to directly extract traditional 3D mesh models. 2) Recovering very fine dynamic details such as clothing wrinkles from monocular videos remains a huge challenge. |
| **(CVPR24)3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting** | A single monocular video, along with a tracked skeleton (fitted SMPL parameters), camera calibration, and foreground masks | pose to create novel animations and the viewpoint for novel view synthesis. | Real-time rendering. Training Time: ~30 mins on a single GPU. | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.09228)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/mikeqzy/3dgs-avatar-release) | 1. The first work to apply 3D Gaussian Splatting for creating animatable human avatars from monocular video.  2. Developed a deformation network with **as-isometric-as-possible regularizations** to handle highly articulated and unseen poses effectively. 3. The first method to simultaneously achieve high-quality rendering, model pose-dependent deformation, fast training (<30 min), and real-time rendering (50+ FPS) in a single framework. | Monocular | NeRFs are too slow for practical applications. | single NVIDIA RTX 3090 GPU | 1.Training Speed:some other grid-based approaches that can train in under 5 minutes. 2.Texture Quality: blurry results on clothing with high-frequency textures or repeating patterns. 3.Geometric Accuracy: does not produce an accurate geometric reconstruction of the avatar's surface. The resulting surface normals can be noisy due to inconsistencies in the depth of the Gaussian splats. |
|**(CVPR24)GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians**| A single monocular RGB video of a person| The body pose of the generated avatar, allowing for realistic animation with out-of-distribution motions | real-time rendering speed of 35 FPS | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.02134)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/aipixel/GaussianAvatar) | 1.  Introduces animatable 3D Gaussians for realistic human avatar modeling from a single video. 2.  Designs a dynamic appearance network with an optimizable feature tensor to capture pose-dependent details like wrinkles. 3.  Proposes a joint optimization of motion and appearance in an end-to-end manner to correct inaccurate motion estimations from monocular videos. | free-viewpoint rendering and novel view synthesis of the animated avatar | information from a single viewpoint is highly limited, and the initial body motion estimations are often inaccurate. | single NVIDIA RTX 3090 GPU | Inaccurate Foreground Segmentation. Difficulty with Loose Clothing | 
|**(CVPR24)Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human Avatar Modeling**| Multi-view RGB videos of a character and their corresponding SMPL-X pose and shape registrations | pose can be controlled by a driving pose signal | 10 FPS for animation when rendering 1024x1024 images on a single RTX 3090 GPU | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2311.16096v3)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/lizhe00/AnimatableGaussians) | 1.  Animatable Gaussians: A new avatar representation combining 3D Gaussian Splatting with powerful 2D CNNs to create lifelike avatars with high-fidelity, pose-dependent dynamics. 2.  Template-guided Parameterization: A method to learn a character-specific template (even for loose clothing like dresses) and parameterize 3D Gaussians onto 2D maps, making them compatible with 2D networks. 3.  Pose Projection Strategy: A PCA-based technique to project novel driving poses into the distribution of training poses, enabling better generalization and higher-quality synthesis for unseen poses. | Multi-view RGB videos are required for initial training and template creation. Experiments show it can work with sparse-view inputs (e.g.,  3 views) and still achieve high-fidelity results. | creating lifelike, animatable human avatars from RGB videos. | Training: single NVIDIA RTX 4090 GPU. Inference/Evaluation: a single NVIDIA RTX 3090 GPU | Coupled Body and Clothing: The method models the human body and clothes as a single, entangled representation. Requires Multi-View Input: The approach relies on multi-view video input to reconstruct the initial high-quality parametric template. Limited Secondary Motion: The model cannot simulate the physical motion of components that are not directly driven by the body's skeleton and joints. |
|**(NeurIPS24)Expressive Gaussian Human Avatars from Monocular RGB Video**|  | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/abs/2407.03204)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/evahuman/EVA_Official) |
|**(NeurIPS24)Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion Models**| A single RGB image of a person. | Stochastic generation of occluded regions | Inference uses a DDIM scheduler with 50 reverse sampling steps. | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://yuxuan-xue.com/human-3diffusion/paper/human-3diffusion.pdf)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/YuxuanSnow/Human3Diffusion/) | 1. A novel image-conditioned generative 3D Gaussian Splatting (3D-GS) model that leverages strong shape and texture priors from 2D multi-view diffusion models. 2. A sophisticated diffusion sampling process that uses the explicit 3D-GS reconstruction at each step to refine the 2D sampling trajectory, enforcing 3D consistency throughout the generation. | The model generates a full 3D representation, which can be rendered from any arbitrary novel viewpoint. Internally. | Creating realistic, high-fidelity 3D human avatars from a single RGB image | Training: 8 NVIDIA A100 GPUs for approximately 5 days. Inference: Not stated. | 1. Resolution: The model is constrained by the 256√ó256 resolution. 2. Challenging Poses: The model may struggle to reconstruct subjects in extremely challenging or unusual poses accurately. | 
|**(ECCV24)Expressive Whole-Body 3D Gaussian Avatar**| | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/abs/2407.21686)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/mks0601/ExAvatar_RELEASE) |
|**(CVPR24)SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded Gaussian Splatting**| monocular videos, along with corresponding pre-processed data like registered mesh templates (e.g., SMPL-X, FLAME), masks, and camera parameters. | animation and pose are explicitly controlled by the underlying triangle mesh. This makes it compatible with various standard animation techniques, including skeletal animation, blend shapes, and direct mesh editing | 300 FPS on a modern desktop GPU (NVIDIA RTX 3090) and 30 FPS on a mobile device (iPhone 13) | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.05087)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/initialneil/SplattingAvatar) |1.  Introduced a novel hybrid representation for avatars that integrates Gaussian Splatting with meshes, achieving both realism and computational efficiency. 2.  Applied "lifted optimization" to avatar modeling, enabling the joint optimization of Gaussian parameters and their trainable mesh embeddings. 3.  Demonstrated state-of-the-art quality and real-time rendering capabilities, including a practical implementation in the Unity game engine| Monocular | creating photorealistic and animatable human avatars that can be rendered in real-time. | | 1.  The final rendering quality is highly dependent on the motion representation capability of the underlying driving mesh. 2.  The model cannot represent movements of elements not explicitly modeled by the driving mesh, such as the independent motion of clothing or hair. |
|**(ICCV25)GUAVA: Generalizable Upper Body 3D Gaussian Avatar**| A single image of a person. | The avatar can be animated and controlled by pose, facial expression, and hand gesture parameters extracted from a target video or image sequence. | real-time. ‚Ä¢ Reconstruction Time: ~0.1 seconds (~98 ms) from a single tracked image. ‚Ä¢ Animation & Rendering Speed: ~50 FPS (52.21 FPS reported). | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/abs/2505.03351)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/Pixel-Talk/GUAVA) | 1. First Framework: Proposes the first framework for creating a generalizable upper-body 3D Gaussian avatar from a single image.  2. Expressive Human Model (EHM): Introduces the EHM, which combines SMPLX and FLAME, along with an accurate tracking method to provide better priors for reconstruction. 3. Efficient Reconstruction: Leverages inverse texture mapping and projection sampling to enable fast, feed-forward inference for reconstructing detailed "Ubody Gaussians". | single view and supports novel view synthesis, allowing the rendered avatar to be seen from different camera angles. |
|**(NeurIPS24)HumanSplat: Generalizable Single-Image Human Gaussian Splatting with Structure Priors**| A single RGB image of a human | static modeling | Reconstruction: ~9.3 seconds on a single NVIDIA A100 GPU (~9s for the diffusion model, ~0.3s for the reconstruction transformer). Rendering: Exceeds 150 FPS.| [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2406.12459)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/humansplat/humansplat) | 1.  The first generalizable method for high-fidelity human reconstruction from a single image using 3D Gaussian Splatting. 2. An end-to-end framework integrating a 2D diffusion model with a latent reconstruction transformer. 3.  Effectively combines appearance priors (from diffusion) and structural priors (from SMPL) within a single Transformer architecture. 4.  Improves reconstruction fidelity in key areas like the face and hands using a semantics-guided hierarchical loss. | Single-view input. Internally, it synthesizes multi-view latent features to perform the reconstruction | To reconstruct a photorealistic 3D human model from a single image, overcoming the need for multi-view data or time-consuming per-instance optimization, while achieving a state-of-the-art balance between quality and speed. | Training: 8x NVIDIA A100 (40G VRAM) GPUs for approximately 4 days. Inference: A single NVIDIA A100 GPU | 1.  Speed could be further improved for real-time applications on mobile devices. 2.  The output is a static model; making it animatable requires separate post-processing steps.|
|**(CVPR25)AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent Gaussian Reconstruction**| A single human image in an arbitrary pose. | The final 3D avatar can be animated using new human pose conditions (e.g., SMPL-X parameters) during inference. | Avatar Creation: Not real-time. It takes about 5 minutes to generate multi-view images and another 5 minutes for 4DGS optimization, totaling approximately 10 minutes on a single RTX-3090 GPU. Animation/Inference: Yes, real-time. Once the avatar is reconstructed, it can be animated and rendered in real-time. | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2412.02684)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/aigc3d/AniGS) not release code | 1. Proposes a multi-view canonical image generation method using a video model trained on in-the-wild video data, avoiding the need for synthetic 3D rigged datasets. 2. Reformulates the problem of 3D reconstruction from inconsistent images as a 4D reconstruction task, introducing an efficient 4D Gaussian Splatting (4DGS) model to handle view inconsistencies. 3. Achieves high-fidelity, animatable avatar generation from a single image, enabling photorealistic and real-time animation during inference | The model generates multi-view images of the subject in a canonical pose (e.g., T-pose) | Generating a high-fidelity, animatable 3D human avatar from a single image. | Training: 16 Nvidia A100 80G GPUs. Inference: A single RTX-3090 GPU. | The optimization process to create an animatable avatar is not real-time and requires several minutes. Future work aims to explore feed-forward reconstruction techniques that are robust to multi-view inconsistencies. |  
|**(CVPR25)GASP: Gaussian Avatars with Synthetic Priors**|  | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2412.07739)<br>  Not Release code |
|**(CVPR25)GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view Diffusion**| A short, monocular video sequence (e.g., 10-15 seconds) captured by a commodity device like a smartphone| Head pose and facial expressions are controllable via the underlying parametric FLAME model. | Rendering is real-time. The method achieves 62 FPS for rendering an image at 802x550 resolution. However, the initial avatar reconstruction/optimization process is offline and time-consuming (approx. 12 hours). | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2412.10209)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/tangjiapeng/GAF) not release code| 1.  A novel approach to reconstruct animatable head avatars from monocular videos by using multi-view head diffusion priors to regularize and complete unobserved regions. 2. A multi-view head diffusion model guided by normal maps rendered from FLAME reconstructions, enabling more precise viewpoint control for generating consistent multi-view images. 3. A strategy to enhance photorealism by using iteratively denoised images (as pseudo-ground truths) and integrating a latent upsampler to refine facial details. | Monocular. The method is specifically designed to work with single-camera videos that have limited viewpoints. |
|**(CVPR25)Creating Your Editable 3D Photorealistic Avatar with Tetrahedron-constrained Gaussian Splatting**| A short (40-50 seconds) monocular RGB video of a person, plus a user-provided text prompt or a reference garment image. | Localized geometry and appearance of the 3D avatar, primarily for editing or trying on different garments  | Training per edit: ‚Ä¢ Localized spatial adaptation (geometry): ~1.2 hours. ‚Ä¢ Texture inpainting (coarse appearance): 20 minutes. ‚Ä¢ Attribute activation (fine appearance): 3 minutes. | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2504.20403)<br>  Not Release code | 1.  A novel system for creating photorealistic, editable 3D avatars from monocular videos using user prompts . 2. TetGS, a novel hybrid representation that combines the controllable structure of tetrahedral grids with the high-fidelity rendering of 3D Gaussian Splatting . 3.  A decoupled editing pipeline that separates the process into localized spatial (geometry) adaptation and appearance generation, enabling stable and high-quality results | Input is a monocular video, but the output is a full 360¬∞ editable 3D avatar. | Addressing the instability and poor quality of the previous 3D avatar editing methods. The goal is to enable precise, localized editing with both geometric adaptability and photorealistic appearance. |  single NVIDIA A40 GPU | 1.  Static Scenes Only: It cannot handle dynamic motion. 2.  Extreme Editing Cases: It may struggle to generate proper geometry when editing from very loose garments (e.g., a dress) to tight-fitting ones. |
|**(CVPR25 Highlight)FRESA: Feedforward Reconstruction of Personalized Skinned Avatars from Few Images**|  | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2503.19207)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/rongakowang/FRESA)Not Release code |
|**(CVPR25 Highlight)Real-time High-fidelity Gaussian Human Avatars with Position-based Interpolation of Spatially Distributed MLPs**| Multi-view videos of a person, along with extracted foreground masks and registered SMPL-X models for 3D pose information | The pose of the human avatar. The model takes a novel pose vector as input to animate the avatar and render it from new viewpoints | Rendering: Real-time at 166 FPS. Training: 17.5 hours for 800K iterations | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2504.12909)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/1231234zhan/mmlphuman) | 1. Spatially Distributed MLPs: A novel architecture where many small MLPs, which only take pose as input, are distributed on the body surface. This drastically reduces computation and enables real-time rendering. 2. Gaussian Offset Basis: A mechanism to learn high-frequency details (e.g., wrinkles, text). It combines smoothly interpolated coefficients with freely learned basis vectors, overcoming the smoothing artifacts of direct interpolation. 3. Control Points: A system to constrain Gaussians to a surface layer by interpolating their position offsets from control points. This improves generalization to novel poses and eliminates artifacts| Multi-view video capture is required. Experiments show it can work with as few as 3 views, up to 39 views for higher quality. | Prior methods for creating animatable Gaussian avatars struggled to balance quality and performance. They were either fast but lacked fine, pose-dependent details, or were high-fidelity but too computationally intensive for real-time rendering (e.g., ~10 FPS). | All evaluations were performed on a single NVIDIA 3090 GPU | 1.  Appearance is solely conditioned on pose, meaning it cannot model complex, non-pose-related cloth dynamics like swaying in the wind.2.  The pipeline relies on a heavy pre-processing setup, including multi-view capture, pose estimation, and template mesh extraction.3.  It currently cannot reconstruct avatars from monocular (single-view) videos|
|**(CVPR25)Vid2Avatar-Pro: Authentic Avatar from Videos in the Wild via Universal Prior** Vid2Avatar(CVPR2023)| A single, monocular "in-the-wild" video of a person | novel human motions (body poses) and rendered from arbitrary novel viewpoints.  |  | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://moygcc.github.io/vid2avatar-pro/static/CVPR2025_Vid2Avatar_Pro.pdf)<br>  Not Release code | 1. A universal prior model for clothed humans learned from a large-scale (1000 identities) high-quality dynamic performance capture dataset.  2. A novel universal prior model architecture using spatially normalized front and back maps as identity conditioning, enabling scalable training across multiple identities. 3. A robust personalization pipeline to create state-of-the-art, photorealistic, and animatable avatars from monocular videos. | Monocular | Addresses the challenges of creating high-quality, animatable 3D avatars from monocular videos where pose diversity and viewpoints are limited. |  | 1. Lack of facial animation. 2. Loose clothing: The model struggles with subjects dressed in very loose garments due to a lack of such data.  3. Lighting conditions: may not perform well under extreme lighting variations. |
|**(CVPR25)WildAvatar: Learning In-the-wild 3D Avatars from the Web**| | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2407.02165)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/wildavatar/WildAvatar_Toolbox) |
|**(CVPR25)IDOL: Instant Photorealistic 3D Human Creation from a Single Image**| A single high-resolution (1024√ó1024) RGB image of a human. | Texture/Appearance: Can be modified by editing the generated UV texture maps. Body Shape: SMPL-X shape parameters. Pose: Animatable (SMPL-X parameters) |  reconstructs a 3D human in under 1 second  | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2412.14963)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/yiyuzhuang/IDOL) | 1. HuGe100K Dataset: A new large-scale (100K subjects, 2.4M+ images) multi-view human dataset with diverse attributes, generated via a scalable pipeline.  2. IDOL Model: An efficient, feed-forward transformer-based model for instant, high-fidelity 3D human reconstruction from a single image. 3. Scalable Framework: Demonstrates that leveraging large-scale generated data significantly enhances model performance and generalizability in 3D human reconstruction. | Input: Single-view. Training Data: Multi-view (24 uniformly sampled views covering a 360¬∞ rotation for each subject). Output: A full 3D model that allows for novel-view synthesis from any viewpoint. | | Training: Performed on a cluster of 32 NVIDIA H100 GPUs for approximately one day. Inference: Performed on a single NVIDIA A100 GPU | 1. Motion Generation: The data generation pipeline is limited to synthesizing single-frame images from fixed viewpoints, not continuous, long motion sequences. 2. Facial Detail: Architecture lacks specific designs for these aspects. 3. Partial Inputs: Handling half-body or other partial inputs remains challenging. |
|**(ICCV25)Sequential Gaussian Avatars with Hierarchical Motion Context**| Multi-view RGB video sequences with corresponding camera parameters and pre-processed SMPL(-X) poses. | avatar can be animated and rendered in novel poses | Training: Fast, e.g., ~25 minutes on ZJU-MoCap dataset.  Rendering: Real-time, ~45 FPS on the I3D-Human dataset, and ~25 FPS on ZJU-MoCap.| [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2411.16768)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/zezeaaa/SeqAvatar) | 1.  Proposed a novel hierarchical motion condition that combines coarse-to-fine human motion (global skeleton poses and localized vertex residuals) to enhance non-rigid deformation prediction. 2. Introduced a spatiotemporal multi-scale sampling strategy to expand the receptive field of the motion context, improving generalization to complex motions. 3. Achieved state-of-the-art rendering performance on challenging datasets with complex motions and garments.| Multi-view. | SMPL-driven 3DGS human avatars struggle to capture fine appearance details (e.g., non-rigid deformation of loose garments) because the mapping from a single static pose to appearance is complex and ambiguous, and these methods often lack temporal motion context. | A single NVIDIA RTX 4090 GPU | 1.  The Gaussian-based representation may introduce slight blur artifacts compared to NeRF's sharper ray-based rendering. 2. The local velocity cues are derived from the coarse SMPL model rather than dense surface tracking, which may limit the accuracy of fine-scale garment deformation modeling.|
|**(ICCV25)GAS: Generative Avatar Synthesis from a Single Image**| A single reference image of a person | novel camera views and novel human poses | 0.40 fps on an NVIDIA A800 GPU. | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2502.06957)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/humansensinglab/GAS) Not Release Full Code | 1. A unified framework for both novel view and pose synthesis that enables training on large-scale, in-the-wild data for better generalization. 2. The use of a dense appearance cue from generalizable NeRF renderings as guidance for the diffusion model, ensuring consistent appearance across views and poses. | novel camera views and novel human poses | Addressing the multi-view and temporal inconsistencies in single-image avatar generation, which arise from the mismatch between sparse conditioning signals (like depth maps) used in prior methods and the subject's true appearance. | Training: 8x A100 GPUs . Inference: A single NVIDIA A800 GPU. |   1. Lacks expressiveness in detailed areas like the face and hands, which can lead to artifacts. 2. The model sometimes struggles to accurately generate complex clothing textures. |
|**(ICCV25 Hightlight)MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar Reconstruction**| A single in-the-wild RGB image. | pose of the avatar | Not specified | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://zj-dong.github.io/MoGA/assets/paper.pdf)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/zj-dong/MoGA) Not Release Code | 1. An optimization-based framework that reconstructs a Gaussian avatar from a single image by fitting a 3D generative model to synthetic views generated by 2D multi-view diffusion. 2. A generative 3D Gaussian avatar prior that provides meaningful initialization, strong 3D regularization, and support for pose refinement during the fitting process. 3. The method generalizes well to in-the-wild images with challenging poses and clothing, and the resulting avatars are animatable without post-processing.| Input: Monocular. Output: Allows for novel view synthesis | Reconstructing high-fidelity, animatable 3D Gaussian avatars from a single image. | Not specified ||
|**(ICCV25)ToMiE: Towards Explicit Exoskeleton for the Reconstruction of Complicated 3D Human Avatars**| Multi-view synchronized videos.|The poses of hand-held objects and the motion of loose-fitting clothing. This control is explicit and fully decoupled from the standard SMPL body pose, allowing for independent animation |Training Time: ~30 minutes. Rendering Speed (FPS): 60+ FPS.| [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2410.08082)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/Yifever20002/ToMiE?tab=readme-ov-file) | 1. Proposes ToMiE, an adaptive growth strategy to create an enhanced SMPL joint tree, decoupling complex parts from the body for state-of-the-art rendering and animation. 2. Introduces a hybrid assignment strategy using LBS weights and Motion Kernels to guide the growth of external joints via gradient localization. 3. Develops a joint optimization approach that fits local rotations across frames for the newly grown joints |  Multi-view synchronized videos. 24 surrounding views for training and 6 novel views for testing | Standard parameterized models like SMPL fail to accurately model and animate 3D humans with hand-held objects or loose-fitting clothing | single GeForce RTX3090 GPU | drastic topological changes, such as a person taking off their clothes or opening a book.|
|**(ICCV25)PERSONA: Personalized Whole-Body 3D Avatar with Pose-Driven Deformations from a Single Image**|  | | | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2508.09973)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/mks0601/PERSONA_RELEASE) |
|**(ICCV25)Fine-Grained 3D Gaussian Head Avatars Modeling from Static Captures via Joint Reconstruction and Registration**|  | | | Not release paper and codes |
|**(ICCV25)Disentangled Clothed Avatar Generation with Layered Representation**| Gaussian noise (for generation). The model is trained on multi-view 2D images | Animation; Camera View Control; Decomposition of body, hair, and clothes; Component Transfer (e.g., swapping clothes, hair, shoes) | Seconds to generate a single avatar | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2501.04631)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://arxiv.org/pdf/2501.04631) | 1. Proposes LayerAvatar, a novel feed-forward diffusion-based pipeline for generating component-disentangled clothed avatars. 2. Introduces a layered UV feature plane representation that facilitates component disentanglement and enhances generation quality. 3. Achieves state-of-the-art performance in generation quality and supports downstream applications like component transfer with high efficiency | Multi-view 2D images are used for training | The challenge is efficiently generating high-quality, customizable 3D clothed avatars. | Training: The model was trained for 6 days on two RTX 3090 GPUs | 1. Performance is sensitive to the accuracy of the estimated segmentation maps and SMPL-X parameters. 2. Potential for collision artifacts between the body and clothing layers. 3. Animation of loose clothing is prone to artifacts. 4. It does not currently handle general accessories like glasses or bags |
|**(ICCV25)LHM: Large Animatable Human Reconstruction Model for Single Image to 3D in Seconds**| A single RGB image of a person | controllable for animation via SMPL-X parameters | 2.01s (LHM-0.5B), 4.13s (LHM-0.7B), and 6.57s (LHM-1B). | [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2503.10625)<br>  [![CODE](https://img.shields.io/badge/CODE-GitHub-8A2BE2.svg)](https://github.com/aigc3d/LHM) | 1. Proposes LHM, a generalizable feed-forward model that reconstructs animatable 3D avatars from a single image in seconds. 2. Introduces a Multimodal Body-Head Transformer (MBHT) to effectively fuse 3D geometric tokens with 2D image features for detailed reconstruction. 3. Achieves state-of-the-art performance in generalization and animation consistency by training on a large-scale video dataset without requiring rigged 3D ground truth data. | single-view image | challenging problem of animatable 3D human reconstruction from a single image | Training: Performed on NVIDIA A100 clusters using 32 to 64 GPUs. Inference: from 18 GB to 24 GB, depending on the model size. | can be affected by the biased distribution of views and poses in the real-world video training dataset |











---

### Pose & Expression Control

| Title | Focus | Controlled Variables | Link | Modality | Contribution | Views Type |
|---|---|---|---|---|---|---|
| SEGA | head / face; disentangle dynamic vs static parts; control expression + view + identity | Expression, view, identity | [arXiv](https://arxiv.org/abs/2504.14373) :contentReference[oaicite:3]{index=3} |
| GAvatar | full‚Äêbody / identity; geometry + appearance; animatable via pose | Pose, identity, geometry detail | [arXiv](https://arxiv.org/abs/2312.11461) :contentReference[oaicite:4]{index=4} |

---


## üîßImplementations & Code

- Whenever available, include GitHub / project URLs  
- Example: if SEGA / GAvatar have official code releases, include here (you may need to check if code is published yet)  
- For projects without code, note that status

---

## üìÇDatasets

- List datasets used by the papers above (e.g. datasets with multi‚Äêview images, expression / pose annotations, identity variation)  
- If there are avatar / human scan datasets useful for 3DGS avatar work, include them

---

## üé•Demos & Videos

- Link to project demo pages  
- Embeddable video / gif if available  
- Screenshots of avatars / animations help

---

## üìñSurveys & Related Resources

- *A Survey on 3D Human Avatar Modeling -- From Reconstruction to Generation* ‚Äî Links: [![PDF](https://img.shields.io/badge/PDF-arXiv-b31b1b.svg)](https://arxiv.org/pdf/2406.04253)

---

## üìãScope / What‚Äôs Included

This repository collects methods and resources that satisfy **all** of:

1. Use **3D Gaussian Splatting** (or methods clearly based on or extending 3DGS) as a core component.  
2. Target **avatars / dynamic humans** (head, face, full body, clothed, expression / pose animation).  
3. Support or demonstrate **dynamic / controllable outputs** (pose, expression, identity, view etc.).  

Excluded (for now):

- Static scenes / static objects without human / avatar elements.  
- Methods that use only 2D Gaussian Splatting without 3D extension.  
- Non-human characters unless specified.  

---

## ü§ùContributing

We welcome contributions! If you find a new paper / code repo / demo / dataset relevant to **3D Gaussian Splatting + avatars / dynamic humans**, please send a Pull Request. When submitting, please include:

- Title, authors, year  
- Type of avatar (head / full body / clothed)  
- Input modality (single image / video / multi-view / text prompt etc.)  
- Controlled variables (pose, expression, identity, view etc.)  
- Speed / resource info (if available)  
- Code / demo / project page link 
---

## ‚ö†Ô∏èLicense

This repository is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

---

## üôèAcknowledgments

Thank you to the authors of all the papers/projects listed, and to the wider 3DGS community.  

---

